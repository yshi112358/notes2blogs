---
title: ã€Pythonã€‘Playwrightã§Manabaã®èª²é¡Œã‚’è‡ªå‹•å–å¾—ã™ã‚‹ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ
tags:
  - Python
  - è‡ªå‹•åŒ–
  - ãƒ‡ãƒ¼ã‚¿åˆ†æ
  - WebScraping
  - playwrite
private: false
updated_at: '2025-07-22T15:41:10+09:00'
id: 4b1dc7223b9f5d4c35e3
organization_url_name: null
slide: false
ignorePublish: false
---
# ã€Pythonã€‘Playwrightã§Manabaã®èª²é¡Œã‚’è‡ªå‹•å–å¾—ã™ã‚‹ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ
## å°å…¥
ç§ã®é€šã£ã¦ã„ã‚‹å¤§å­¦ã§ã¯Manabaã¨ã„ã†ã‚µã‚¤ãƒˆã§èª²é¡Œã®æå‡ºã‚„æˆæ¥­æƒ…å ±ã®ç¢ºèªãŒã§ãã¾ã™ã€‚
ã—ã‹ã—ã€ã“ã®ManabaãŒã¾ã‚ä½¿ã„ã«ãã„ã€‚ç‰¹ã«å¤–éƒ¨é€£æºãŒå……å®Ÿã—ã¦ã„ãªã„ã®ã§ã€èª²é¡Œã®ãƒªãƒã‚¤ãƒ³ãƒ‰ãŒã§ããªã„ã€‚
ãã“ã§ã€ã“ã®Manabaã®èª²é¡Œã‚’è‡ªå‹•ã§å–å¾—ã™ã‚‹æ–¹æ³•ã‚’æ¨¡ç´¢ã—ã¾ã—ãŸã€‚

## çµè«–
**Playwrightã¨Webã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°æŠ€è¡“**ã‚’æ´»ç”¨ã—ã€èª²é¡Œæƒ…å ±ã‚’jsonãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦å–å¾—ã™ã‚‹ã“ã¨ãŒã§ãã¾ã—ãŸã€‚

## çµŒç·¯
HTTP Requestã§å‡ºæ¥ãªã„ã‹
â†’ å¤§å­¦ã®ã‚ˆã†ãªã‚µã‚¤ãƒˆã¯SAMLèªè¨¼ã¨ã„ã†ä¸€åº¦é™ã‚Šã®èªè¨¼éµã§ãƒ­ã‚°ã‚¤ãƒ³ã™ã‚‹æ–¹å¼ã«ã‚ˆã‚Šä¸å¯
â†’ Cookieã‚’ä½¿ã†æ–¹æ³•ã‚‚ã‚ã‚‹ãŒã€ã§ãã‚Œã°æŒç¶šã—ã¦ãƒ­ã‚°ã‚¤ãƒ³ã§ãã‚‹ã‚ˆã†ãªæ–¹å¼ã«ã—ãŸã„
åˆ¥ã®æ–¹æ³•ã‚’æ¨¡ç´¢
â†’ Playwriteã‚’ä½¿ã£ã¦ã•ã‚‚äººé–“ãŒãƒ­ã‚°ã‚¤ãƒ³ã—ã¦ã„ã‚‹ã®ã¨å¤‰ã‚ã‚‰ãªã„ç’°å¢ƒã§è‡ªå‹•åŒ–ã—ã‚ˆã†ï¼

## ä»Šå¾Œã®å±•é–‹
èª²é¡Œã‚’å–å¾—ã§ããŸã®ã§ã€ã“ã‚Œã‚’å…ƒã«TODOãƒªã‚¹ãƒˆã«è¿½åŠ ã™ã‚‹ã€‚ãã®éƒ¨åˆ†ã®è‡ªå‹•åŒ–ã«ã¯n8nã‚’åˆ©ç”¨äºˆå®šã€‚
ã¾ãšã¯Macbookä¸Šã®ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã§å‹•ã‹ã™ãŒã€ãƒ‘ã‚½ã‚³ãƒ³ãŒä½™ã£ã¦ã„ãŸã‚‰ãã‚Œã§ã‚µãƒ¼ãƒãƒ¼ã‚’å»ºã¦å®Ÿè£…ã—ãŸã„ã§ã™ã­ã€‚

## å¾¡ç¤¼
ä¸‹ã—æ›¸ãã«ã¯ãªã£ã¦ã—ã¾ã„ã¾ã—ãŸãŒã€è‡ªå‹•åŒ–ã‚’ã™ã‚‹ã«ã‚ãŸã£ã¦æœ‰ç”¨ãªãƒ„ãƒ¼ãƒ«ã ã¨æ€ã„ã¾ã™ã€‚ä»Šå¾Œã¯é‡‘èç³»ã®ãƒ„ãƒ¼ãƒ«ã‚‚ã“ã‚Œã§ä½œã‚Œã‚Œã°ãªã¨æ€ã£ã¦ã„ã¾ã™ãŒã€æœãŸã—ã¦â€¦
ä»¥ä¸‹ä½œã‚Šæ–¹ã«ãªã‚Šã¾ã™ãŒã€å‚è€ƒã¾ã§ã«ï¼

---

# ä½œã‚Šæ–¹
## ğŸ› ï¸ é–‹ç™ºç’°å¢ƒãƒ»å‰ææ¡ä»¶

```bash
# Pythonç’°å¢ƒ
Python 3.8ä»¥ä¸Š

# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒª
pip install playwright
playwright install chromium

# ãã®ä»–æ¨™æº–ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
json, os, time, urllib.parse, datetime, csv
```

## ğŸ“‹ ã‚·ã‚¹ãƒ†ãƒ æ§‹æˆ

### å–å¾—å¯èƒ½ãƒ‡ãƒ¼ã‚¿
- **èª²é¡ŒåŸºæœ¬æƒ…å ±**: ã‚¿ã‚¤ãƒˆãƒ«ã€ã‚³ãƒ¼ã‚¹ã€ç· åˆ‡æ—¥æ™‚
- **èª²é¡Œè©³ç´°**: èª¬æ˜æ–‡ã€æå‡ºè¦ä»¶ã€è©•ä¾¡åŸºæº–
- **æå‡ºæ–¹æ³•**: ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰/ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›/é¸æŠå¼
- **æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«**: PDFã€Wordã€Excelç­‰ã®ãƒªãƒ³ã‚¯
- **çµ±è¨ˆæƒ…å ±**: èª²é¡Œã‚¿ã‚¤ãƒ—åˆ¥ãƒ»ã‚³ãƒ¼ã‚¹åˆ¥é›†è¨ˆ

### å‡ºåŠ›å½¢å¼
- **è©³ç´°JSON**: å…¨ãƒ‡ãƒ¼ã‚¿ã®æ§‹é€ åŒ–ä¿å­˜
- **çµ±è¨ˆJSON**: åˆ†æç”¨ã‚µãƒãƒªãƒ¼æƒ…å ±
- **CSV**: Excelç­‰ã§ã®åˆ†æç”¨ãƒ†ãƒ¼ãƒ–ãƒ«

## ğŸš€ å®Ÿè£…ã‚³ãƒ¼ãƒ‰

### ãƒ¡ã‚¤ãƒ³ã‚¯ãƒ©ã‚¹å®šç¾©

```python
import json
import os
import time
from playwright.sync_api import sync_playwright
from urllib.parse import urljoin
from datetime import datetime

class TaskScraperFull:
    def __init__(self):
        self.tasks_data = {}
        self.output_dir = "/path/to/output/tasks_data"  # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        os.makedirs(self.output_dir, exist_ok=True)
        self.success_count = 0
        self.error_count = 0
```

### 1. manabaã¸ã®è‡ªå‹•ãƒ­ã‚°ã‚¤ãƒ³

```python
def login_to_manaba(self, page):
    """manabaã«ãƒ­ã‚°ã‚¤ãƒ³"""
    print("ğŸ” manabaã«ãƒ­ã‚°ã‚¤ãƒ³ã—ã¦ã„ã¾ã™...")
    page.goto("https://room.chuo-u.ac.jp/")
    page.wait_for_timeout(3000)
    
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼IDãƒ»ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰å…¥åŠ›
    page.fill("input[name='username']", "YOUR_USER_ID")
    page.fill("input[name='password']", "YOUR_PASSWORD")
    page.click("#login_button")
    page.wait_for_timeout(5000)
    print("âœ… ãƒ­ã‚°ã‚¤ãƒ³å®Œäº†")
```

**ãƒã‚¤ãƒ³ãƒˆ**:
- `page.wait_for_timeout()`ã§ãƒšãƒ¼ã‚¸èª­ã¿è¾¼ã¿å®Œäº†ã‚’å¾…æ©Ÿ
- ã‚»ãƒ¬ã‚¯ã‚¿ã¯é–‹ç™ºè€…ãƒ„ãƒ¼ãƒ«ã§äº‹å‰ã«èª¿æŸ»
- ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã§ãƒ­ã‚°ã‚¤ãƒ³å¤±æ•—ã‚’æ¤œå‡ºå¯èƒ½

### 2. èª²é¡Œä¸€è¦§ã®å–å¾—

```python
def extract_task_list(self, page):
    """èª²é¡Œä¸€è¦§ã‚’å–å¾—"""
    print("ğŸ“‹ èª²é¡Œä¸€è¦§ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹ã—ã¦ã„ã¾ã™...")
    page.goto("https://room.chuo-u.ac.jp/ct/home_library_query")
    page.wait_for_timeout(5000)

    tasks = []
    table_rows = page.query_selector_all("table tr")
    headers = []
    
    for row_index, row in enumerate(table_rows):
        cells = row.query_selector_all("td, th")
        cell_texts = [cell.inner_text().strip() for cell in cells]
        
        # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œæ¤œå‡º
        header_keywords = ["Type", "ã‚¿ã‚¤ãƒ—", "Title", "ã‚¿ã‚¤ãƒˆãƒ«"]
        if any(keyword in text for text in cell_texts for keyword in header_keywords):
            headers = cell_texts
            continue
            
        # ãƒ‡ãƒ¼ã‚¿è¡Œå‡¦ç†ï¼ˆ6åˆ—ã®å ´åˆï¼‰
        task_types = ["ãƒ¬ãƒãƒ¼ãƒˆ", "ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆ", "å°ãƒ†ã‚¹ãƒˆ", "ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ"]
        if len(cell_texts) == 6 and headers and cell_texts[0] in task_types:
            # é‡è¦: ã‚¿ã‚¤ãƒˆãƒ«éƒ¨åˆ†ã®ãƒªãƒ³ã‚¯ã‚’å–å¾—
            title_links = row.query_selector_all("a")
            task_link = None
            
            for link in title_links:
                href = link.get_attribute("href")
                link_text = link.inner_text().strip()
                
                # ã‚¿ã‚¤ãƒˆãƒ«ã¨å®Œå…¨ä¸€è‡´ã™ã‚‹ãƒªãƒ³ã‚¯ã‚’æ¢ã™
                if href and link_text == cell_texts[1]:
                    task_link = href
                    break
            
            task_info = {
                "type": cell_texts[0],
                "title": cell_texts[1],
                "course": cell_texts[2],
                "start_time": cell_texts[3],
                "end_time": cell_texts[4],
                "period": cell_texts[5],
                "absolute_url": urljoin(page.url, task_link) if task_link else None
            }
            
            tasks.append(task_info)
    
    return tasks
```

**é‡è¦ãªç™ºè¦‹**:
manabaã§ã¯å„è¡Œã«è¤‡æ•°ã®ãƒªãƒ³ã‚¯ãŒå­˜åœ¨ã—ã¾ã™ï¼š
- ãƒªãƒ³ã‚¯0: **ã‚¿ã‚¤ãƒ—éƒ¨åˆ†** â†’ èª²é¡Œã®ãƒªã‚¹ãƒˆãƒšãƒ¼ã‚¸
- ãƒªãƒ³ã‚¯1: **ã‚¿ã‚¤ãƒˆãƒ«éƒ¨åˆ†** â†’ å€‹åˆ¥èª²é¡Œè©³ç´°ãƒšãƒ¼ã‚¸ âœ…

**ã‚¿ã‚¤ãƒˆãƒ«éƒ¨åˆ†ã®ãƒªãƒ³ã‚¯**ã‹ã‚‰é·ç§»ã™ã‚‹ã“ã¨ã§ã€æ­£ç¢ºãªèª²é¡Œè©³ç´°ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã§ãã¾ã™ã€‚

### 3. å€‹åˆ¥èª²é¡Œã®è©³ç´°å–å¾—

```python
def extract_task_details(self, page, task_info):
    """å€‹åˆ¥ã®èª²é¡Œè©³ç´°ã‚’å–å¾—"""
    try:
        print(f"ğŸ“„ èª²é¡Œè©³ç´°å–å¾—ä¸­: {task_info['title']}")
        page.goto(task_info["absolute_url"], timeout=60000)
        page.wait_for_timeout(3000)

        # ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã‚’æº–å‚™
        task_details = {
            "basic_info": task_info,
            "page_title": page.title(),
            "description": "",
            "submission_info": [],
            "files": [],
            "deadline_info": {},
            "full_content": page.inner_text("body")
        }

        # å¤šæ®µéšãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ–¹å¼ã§èª¬æ˜æ–‡ã‚’å–å¾—
        description_selectors = [
            ".assignment-description", ".report-description", 
            ".content-body", ".task-content", ".pagebody",
            "main", ".main-content"
        ]
        
        for selector in description_selectors:
            element = page.query_selector(selector)
            if element:
                desc_text = element.inner_text().strip()
                if len(desc_text) > 50:  # æ„å‘³ã®ã‚ã‚‹å†…å®¹ã‹ãƒã‚§ãƒƒã‚¯
                    task_details["description"] = desc_text
                    break

        # æå‡ºæ–¹æ³•ã®è©³ç´°åˆ†æ
        submission_elements = page.query_selector_all(
            "input[type='file'], textarea, select, input[type='text']"
        )
        
        for element in submission_elements:
            tag_name = element.evaluate("el => el.tagName.toLowerCase()")
            element_info = {
                "tag": tag_name,
                "type": element.get_attribute("type") or "",
                "name": element.get_attribute("name") or "",
                "required": element.get_attribute("required") or False
            }
            
            # æå‡ºå½¢å¼ã‚’åˆ†é¡
            if tag_name == "input" and element.get_attribute("type") == "file":
                element_info["submission_type"] = "file_upload"
            elif tag_name == "textarea":
                element_info["submission_type"] = "text_area"
            elif tag_name == "select":
                element_info["submission_type"] = "selection"
            
            task_details["submission_info"].append(element_info)

        # æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«ã®å–å¾—
        file_selectors = [
            "a[href*='download']", "a[href*='.pdf']", 
            "a[href*='.doc']", "a[href*='.xlsx']"
        ]
        
        files = []
        for selector in file_selectors:
            file_links = page.query_selector_all(selector)
            for link in file_links:
                href = link.get_attribute("href")
                if href:
                    files.append({
                        "name": link.inner_text().strip(),
                        "url": urljoin(page.url, href),
                        "type": self.get_file_type(href)
                    })
        
        task_details["files"] = files
        self.success_count += 1
        return task_details

    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {str(e)}")
        self.error_count += 1
        return {"error": str(e), "basic_info": task_info}
```

### 4. ãƒ‡ãƒ¼ã‚¿ä¿å­˜ã¨çµ±è¨ˆç”Ÿæˆ

```python
def save_data(self):
    """å¤šå½¢å¼ã§ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # 1. è©³ç´°JSONãƒ‡ãƒ¼ã‚¿
    main_file = os.path.join(self.output_dir, f"all_tasks_{timestamp}.json")
    with open(main_file, "w", encoding="utf-8") as f:
        json.dump(self.tasks_data, f, indent=2, ensure_ascii=False)
    
    # 2. çµ±è¨ˆæƒ…å ±JSON
    stats = {
        "total_tasks": len(self.tasks_data),
        "success_count": self.success_count,
        "error_count": self.error_count,
        "success_rate": f"{(self.success_count / len(self.tasks_data) * 100):.1f}%",
        "task_types": {},
        "courses": {},
        "submission_methods": {}
    }
    
    # çµ±è¨ˆãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆ
    for task_data in self.tasks_data.values():
        if "basic_info" in task_data:
            task_type = task_data["basic_info"].get("type", "Unknown")
            course = task_data["basic_info"].get("course", "Unknown")
            stats["task_types"][task_type] = stats["task_types"].get(task_type, 0) + 1
            stats["courses"][course] = stats["courses"].get(course, 0) + 1
    
    stats_file = os.path.join(self.output_dir, f"stats_{timestamp}.json")
    with open(stats_file, "w", encoding="utf-8") as f:
        json.dump(stats, f, indent=2, ensure_ascii=False)
    
    # 3. CSVã‚µãƒãƒªãƒ¼
    import csv
    csv_file = os.path.join(self.output_dir, f"summary_{timestamp}.csv")
    with open(csv_file, "w", encoding="utf-8", newline="") as f:
        writer = csv.writer(f)
        writer.writerow([
            "ã‚¿ã‚¤ãƒ—", "ã‚¿ã‚¤ãƒˆãƒ«", "ã‚³ãƒ¼ã‚¹", "ç· åˆ‡æ—¥æ™‚", 
            "æå‡ºå½¢å¼", "æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«æ•°", "èª¬æ˜æ–‡å­—æ•°"
        ])
        
        for task_data in self.tasks_data.values():
            if "basic_info" in task_data:
                info = task_data["basic_info"]
                files_count = len(task_data.get("files", []))
                desc_length = len(task_data.get("description", ""))
                submission_types = [s.get("submission_type", "") 
                                  for s in task_data.get("submission_info", [])]
                
                writer.writerow([
                    info.get("type", ""),
                    info.get("title", ""),
                    info.get("course", ""),
                    info.get("end_time", ""),
                    ", ".join(submission_types),
                    files_count,
                    desc_length
                ])
```

### 5. ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œéƒ¨åˆ†

```python
def run(self):
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œãƒ¡ã‚½ãƒƒãƒ‰"""
    print("ğŸš€ èª²é¡Œãƒ‡ãƒ¼ã‚¿å–å¾—ã‚’é–‹å§‹ã—ã¾ã™")
    
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)  # é«˜é€ŸåŒ–ã®ãŸã‚éè¡¨ç¤º
        page = browser.new_page()

        try:
            # ãƒ­ã‚°ã‚¤ãƒ³
            self.login_to_manaba(page)

            # èª²é¡Œä¸€è¦§å–å¾—
            tasks_list = self.extract_task_list(page)
            total_tasks = len(tasks_list)
            print(f"ğŸ“‹ ç™ºè¦‹ã•ã‚ŒãŸèª²é¡Œæ•°: {total_tasks}")

            # å„èª²é¡Œã®è©³ç´°å–å¾—
            for i, task_info in enumerate(tasks_list, 1):
                print(f"[{i}/{total_tasks}] {task_info['title']}")
                
                task_details = self.extract_task_details(page, task_info)
                task_id = f"{task_info['course']}_{task_info['title']}_{i}"
                self.tasks_data[task_id] = task_details
                
                # é€²æ—è¡¨ç¤º
                progress = (i / total_tasks) * 100
                print(f"é€²æ—: {progress:.1f}% (æˆåŠŸ:{self.success_count}, ã‚¨ãƒ©ãƒ¼:{self.error_count})")
                
                time.sleep(2)  # ã‚µãƒ¼ãƒãƒ¼è² è·è»½æ¸›

        except Exception as e:
            print(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        finally:
            browser.close()
            if self.tasks_data:
                self.save_data()

def main():
    scraper = TaskScraperFull()
    scraper.run()

if __name__ == "__main__":
    main()
```

## ğŸ’¡ æŠ€è¡“çš„ãƒã‚¤ãƒ³ãƒˆ

### 1. é©åˆ‡ãªã‚»ãƒ¬ã‚¯ã‚¿é¸æŠ
```python
# âŒ é–“é•ã£ãŸæ–¹æ³•
link = row.query_selector("a")  # æœ€åˆã®ãƒªãƒ³ã‚¯ã‚’å–å¾—

# âœ… æ­£ã—ã„æ–¹æ³•  
for link in title_links:
    if link.inner_text().strip() == cell_texts[1]:  # ã‚¿ã‚¤ãƒˆãƒ«ä¸€è‡´ç¢ºèª
        task_link = href
```

### 2. å¤šæ®µéšãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
```python
# è¤‡æ•°ã®ã‚»ãƒ¬ã‚¯ã‚¿ã§èª¬æ˜æ–‡ã‚’è©¦è¡Œ
description_selectors = [
    ".assignment-description",  # æœ€å„ªå…ˆ
    ".content-body",           # æ¬¡å–„
    ".pagebody"               # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
]
```

### 3. ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
```python
try:
    page.goto(url, timeout=60000)  # 60ç§’ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
except Exception as e:
    self.error_count += 1
    return {"error": str(e)}
```

## ğŸ”’ ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ»æ³¨æ„äº‹é …

1. **èªè¨¼æƒ…å ±ã®ç®¡ç†**
   ```python
   # ç’°å¢ƒå¤‰æ•°ã§ã®ç®¡ç†æ¨å¥¨
   import os
   username = os.getenv('MANABA_USERNAME')
   password = os.getenv('MANABA_PASSWORD')
   ```

2. **ã‚µãƒ¼ãƒãƒ¼è² è·ã¸ã®é…æ…®**
   ```python
   time.sleep(2)  # é©åˆ‡ãªé–“éš”ã§ã‚¢ã‚¯ã‚»ã‚¹
   ```

3. **åˆ©ç”¨è¦ç´„ã®éµå®ˆ**
   - å¤§å­¦ã®æƒ…å ±ã‚·ã‚¹ãƒ†ãƒ åˆ©ç”¨è¦ç´„ã«å¾“ã†
   - å€‹äººåˆ©ç”¨ç›®çš„ã§ã®ä½¿ç”¨ã«ç•™ã‚ã‚‹
   - éåº¦ãªè² è·ã‚’ã‹ã‘ãªã„

## ğŸ“ˆ å¿œç”¨ãƒ»æ‹¡å¼µå¯èƒ½æ€§

### ãƒ‡ãƒ¼ã‚¿åˆ†æã¸ã®æ´»ç”¨
- **ç· åˆ‡ç®¡ç†**: å„ªå…ˆåº¦é †ã®èª²é¡Œãƒªã‚¹ãƒˆç”Ÿæˆ
- **å­¦ç¿’è¨ˆç”»**: ã‚³ãƒ¼ã‚¹åˆ¥ãƒ»æœŸé™åˆ¥ã®å­¦ç¿’ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«
- **çµ±è¨ˆåˆ†æ**: èª²é¡Œæå‡ºçŠ¶æ³ãƒ»æˆç¸¾åˆ†æ

---

*ã“ã®è¨˜äº‹ãŒåŒã˜ã‚ˆã†ãªèª²é¡Œã‚’æŠ±ãˆã‚‹å­¦ç”Ÿã‚„é–‹ç™ºè€…ã®æ–¹ã®å‚è€ƒã«ãªã‚Œã°å¹¸ã„ã§ã™ã€‚è³ªå•ã‚„ã”æ„è¦‹ãŒã”ã–ã„ã¾ã—ãŸã‚‰ã€ã‚³ãƒ¡ãƒ³ãƒˆã§ãŠèã‹ã›ãã ã•ã„ï¼*

## ğŸ·ï¸ ã‚¿ã‚°
`Python` `Playwright` `WebScraping` `è‡ªå‹•åŒ–` `ãƒ‡ãƒ¼ã‚¿åˆ†æ` `manaba` `ä¸­å¤®å¤§å­¦` 
